{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.cpp_extension\n",
    "from pathlib import Path\n",
    "import random\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_correctness(softmax_impl, repeats=100):\n",
    "    for i in range(repeats):\n",
    "        row = random.randint(1, 1024 * 32)\n",
    "        cols = random.randint(1, 1024 * 32)\n",
    "        a = torch.randn((row, cols), device=\"cuda\", dtype=torch.float32)\n",
    "        torch.testing.assert_close(softmax_impl(a), torch.nn.functional.softmax(a, dim=-1), msg=f\"row = {row}, cols = {cols}\")\n",
    "    return softmax_impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_correctness(lambda a: torch.nn.functional.softmax(a, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cuda_impl(file_name):\n",
    "    cuda_src = Path(file_name).read_text()\n",
    "    cpp_src = \"\"\"\n",
    "    torch::Tensor softmax(const torch::Tensor& x);\n",
    "    \"\"\"\n",
    "    my = torch.utils.cpp_extension.load_inline(\n",
    "        \"my\", cpp_src, cuda_src,\n",
    "        functions=['softmax'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True,\n",
    "    )\n",
    "    return my.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kernel(name, kernel):\n",
    "    kernels[name] = kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_kernel(\"Torch\", torch.nn.functional.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_pytorch_softmax(x, dim=None):\n",
    "    m = torch.max(x, dim=dim, keepdim=True)[0]\n",
    "    e = torch.exp(x - m)\n",
    "    s = torch.sum(e, dim=dim, keepdim=True)\n",
    "    return e / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_kernel(\"Naive\", test_correctness(lambda a: naive_pytorch_softmax(a, dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def compiled_naive_pytorch_softmax(x, dim=None):\n",
    "    return naive_pytorch_softmax(x, dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_kernel(\"CompiledNaive\", test_correctness(lambda a: compiled_naive_pytorch_softmax(a, dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax1 = add_kernel(\"CUDA1\", test_correctness(load_cuda_impl(\"softmax1.cu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax2 = add_kernel(\"CUDA2\", test_correctness(load_cuda_impl(\"softmax2.cu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax2_shfl = add_kernel(\"CUDA2_SHFL\", test_correctness(load_cuda_impl(\"softmax2_shfl.cu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax3 = add_kernel(\"CUDA3\", test_correctness(load_cuda_impl(\"softmax3.cu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax3_shfl = add_kernel(\"CUDA3_SHFL\", test_correctness(load_cuda_impl(\"softmax3_shfl.cu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax5 = add_kernel(\"CUDA5\", test_correctness(load_cuda_impl(\"softmax5.cu\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def triton_softmax1_kernel(\n",
    "    x,\n",
    "    x_row_stride,\n",
    "    y,\n",
    "    y_row_stride,\n",
    "    n_cols,\n",
    "    block_size: tl.constexpr,\n",
    "):\n",
    "    row_idx = tl.program_id(0)\n",
    "    x_row_start = x + row_idx * x_row_stride\n",
    "    col_offsets = tl.arange(0, block_size)\n",
    "    row_ptrs = x_row_start + col_offsets\n",
    "    mask = col_offsets < n_cols\n",
    "    row = tl.load(row_ptrs, mask=mask, other=float('-inf'))\n",
    "\n",
    "    row_max = tl.max(row, axis=0)\n",
    "    safe_row = row - row_max\n",
    "    exp_row = tl.exp(safe_row)\n",
    "    sum_row = tl.sum(exp_row, axis=0)\n",
    "    y_row_start = y + row_idx * y_row_stride\n",
    "    y_ptrs = y_row_start + col_offsets\n",
    "    tl.store(y_ptrs, exp_row / sum_row, mask=mask)\n",
    "\n",
    "def triton_softmax1(x: torch.Tensor) -> torch.Tensor:\n",
    "    x_flatten = x.reshape(-1, x.size(-1))\n",
    "    y = torch.empty_like(x_flatten)\n",
    "    rows, cols = x_flatten.shape\n",
    "    block_size = triton.next_power_of_2(cols)\n",
    "    num_warps = 4\n",
    "    if block_size > 2047:\n",
    "        num_warps = 8\n",
    "    elif block_size > 4095:\n",
    "        num_warps = 16\n",
    "\n",
    "    grid = (rows, )\n",
    "    triton_softmax1_kernel[grid](x_flatten, x_flatten.stride(0), y, y.stride(0), cols, block_size, num_warps=num_warps)\n",
    "    \n",
    "    return y.view_as(x)\n",
    "\n",
    "add_kernel(\"Triton1\", test_correctness(triton_softmax1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def triton_softmax2_kernel(\n",
    "    x,\n",
    "    x_row_stride,\n",
    "    y,\n",
    "    y_row_stride,\n",
    "    n_cols,\n",
    "    block_size: tl.constexpr,\n",
    "):\n",
    "    row_idx = tl.program_id(0)\n",
    "    x_row_start = x + row_idx * x_row_stride\n",
    "    y_row_start = y + row_idx * y_row_stride\n",
    "    offsets = tl.arange(0, block_size)\n",
    "    row_max = tl.full((), -float(\"inf\"), dtype=tl.float32)\n",
    "    for b in range(0, (n_cols + block_size - 1) // block_size):\n",
    "        idx = b * block_size + offsets\n",
    "        mask = idx < n_cols\n",
    "        x_vals = tl.load(x_row_start + idx, mask=mask)\n",
    "        row_max = tl.maximum(row_max, tl.max(x_vals.to(tl.float32)))\n",
    "\n",
    "    row_sum = tl.zeros((), dtype=tl.float32)\n",
    "    for b in range(0, (n_cols + block_size - 1) // block_size):\n",
    "        idx = b * block_size + offsets\n",
    "        mask = idx < n_cols\n",
    "        x_vals = tl.load(x_row_start + idx, mask=mask, other=-float(\"inf\"))\n",
    "        x_vals = tl.exp(x_vals - row_max)\n",
    "        row_sum += tl.sum(x_vals)\n",
    "        tl.store(y_row_start + idx, x_vals, mask=mask)\n",
    "\n",
    "    for b in range(0, (n_cols + block_size - 1) // block_size):\n",
    "        idx = b * block_size + offsets\n",
    "        mask = idx < n_cols\n",
    "        y_vals = tl.load(y_row_start + idx, mask=mask)\n",
    "        y_vals /= row_sum\n",
    "        tl.store(y_row_start + idx, y_vals, mask=mask)\n",
    "    \n",
    "\n",
    "def triton_softmax2(x: torch.Tensor) -> torch.Tensor:\n",
    "    x_flatten = x.reshape(-1, x.size(-1))\n",
    "    y = torch.empty_like(x_flatten)\n",
    "    rows, cols = x_flatten.shape\n",
    "    block_size = 1024 * 32\n",
    "    num_warps = 4\n",
    "    if block_size > 2047:\n",
    "        num_warps = 8\n",
    "    elif block_size > 4095:\n",
    "        num_warps = 16\n",
    "    grid = (rows,)\n",
    "    triton_softmax2_kernel[grid](x_flatten, x_flatten.stride(0), y, y.stride(0), cols, block_size, num_warps=num_warps)\n",
    "    return y.view_as(x)\n",
    "\n",
    "add_kernel(\"Triton2\", test_correctness(triton_softmax2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def triton_softmax3_kernel(\n",
    "    x,\n",
    "    x_row_stride,\n",
    "    y,\n",
    "    y_row_stride,\n",
    "    n_cols,\n",
    "    block_size: tl.constexpr,\n",
    "):\n",
    "    row_idx = tl.program_id(0)\n",
    "    x_row_start = x + row_idx * x_row_stride\n",
    "    y_row_start = y + row_idx * y_row_stride\n",
    "    current_max = float('-inf')\n",
    "    current_sum = 0.0\n",
    "    \n",
    "    for block_offset in tl.range(0, n_cols, block_size):\n",
    "        col_offsets = block_offset + tl.arange(0, block_size)\n",
    "        block = tl.load(x_row_start + col_offsets, mask=col_offsets < n_cols, other=float('-inf'))\n",
    "        new_max = tl.maximum(current_max, tl.max(block))\n",
    "        current_sum *= tl.exp(current_max - new_max)\n",
    "        current_sum += tl.sum(tl.exp(block - new_max))\n",
    "        current_max = new_max\n",
    "\n",
    "    for block_offset in tl.range(0, n_cols, block_size):\n",
    "        col_offsets = block_offset + tl.arange(0, block_size)\n",
    "        block = tl.load(x_row_start + col_offsets, mask=col_offsets < n_cols, other=float('-inf'))\n",
    "        tl.store(y_row_start + col_offsets, tl.exp(block - current_max) / current_sum, mask=col_offsets < n_cols)\n",
    "\n",
    "\n",
    "def triton_softmax3(x: torch.Tensor) -> torch.Tensor:\n",
    "    x_flatten = x.reshape(-1, x.size(-1))\n",
    "    y = torch.empty_like(x_flatten)\n",
    "    rows, cols = x_flatten.shape\n",
    "    block_size = 32 * 1024\n",
    "    num_warps = 32\n",
    "    grid = (rows, )\n",
    "    triton_softmax3_kernel[grid](x_flatten, x_flatten.stride(0), y, y.stride(0), cols, block_size, num_warps=num_warps)\n",
    "    return y.view_as(x)\n",
    "\n",
    "add_kernel(\"Triton3\", test_correctness(triton_softmax3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['size'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(12, 20, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=list(kernels.keys()),  # Possible values for `line_arg`.\n",
    "        line_names=list(kernels.keys()),  # Label name for the lines.\n",
    "        # styles=[('blue', '-'), ('green', '-'), ('red', '-'), ('pink', '-'), ('yellow', '-'), ('black', '-'), ('orange', '-'), ('purple', '-'), ('cyan', '-'), ('gray', '-'), ('gray', '--')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='softmax-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def triton_softmax_benchmark(size, provider):\n",
    "    x = torch.rand((1, size), device=\"cuda\", dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    ms, min_ms, max_ms = triton.testing.do_bench(lambda: kernels[provider](x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_softmax_benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['size'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(12, 30, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=[\"CUDA5\", \"Naive\"],  # Possible values for `line_arg`.\n",
    "        line_names=[\"CUDA5\", \"Naive\"],  # Label name for the lines.\n",
    "        # styles=[('blue', '-'), ('green', '-'), ('red', '-'), ('pink', '-'), ('yellow', '-'), ('black', '-'), ('orange', '-'), ('purple', '-'), ('cyan', '-'), ('gray', '-'), ('gray', '--')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='softmax-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def triton_softmax_benchmark(size, provider):\n",
    "    x = torch.rand((1, size), device=\"cuda\", dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    ms, min_ms, max_ms = triton.testing.do_bench(lambda: kernels[provider](x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_softmax_benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
